{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建计算图-LSTM\n",
    "#     embedding\n",
    "#     LSTM\n",
    "#     fc\n",
    "#     train_op\n",
    "# 训练流程代码\n",
    "# 数据集封装\n",
    "#     api: next_batch(batch_size)\n",
    "# 词表封装:\n",
    "#     api: sentenceToId(text_sentence):句子转换id\n",
    "# 类别封装:\n",
    "#     api: categoryToId(text_category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\zyc\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class HParams:\n",
    "    def __init__(self,\n",
    "                 num_embedding_size,\n",
    "                 num_timesteps,\n",
    "                 num_lstm_nodes,\n",
    "                 num_lstm_layers,\n",
    "                 num_fc_nodes,\n",
    "                 batch_size,\n",
    "                 clip_lstm_grads,\n",
    "                 learning_rate,\n",
    "                 # 字符出现的阈值\n",
    "                 num_word_threshold):\n",
    "        self._num_embedding_size = num_embedding_size\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._num_lstm_nodes = num_lstm_nodes\n",
    "        self._num_lstm_layers = num_lstm_layers\n",
    "        self._num_fc_nodes = num_fc_nodes\n",
    "        self._batch_size = batch_size\n",
    "        self._clip_lstm_grads = clip_lstm_grads\n",
    "        self._learning_rate = learning_rate\n",
    "        # 字符出现的阈值\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "\n",
    "    @property\n",
    "    def num_embedding_size(self):\n",
    "        return self._num_embedding_size\n",
    "    @property\n",
    "    def num_timesteps(self):\n",
    "        return self._num_timesteps\n",
    "    @property\n",
    "    def num_lstm_nodes(self):\n",
    "        return self._num_lstm_nodes\n",
    "    @property\n",
    "    def num_lstm_layers(self):\n",
    "        return self._num_lstm_layers\n",
    "    @property\n",
    "    def num_fc_nodes(self):\n",
    "        return self._num_fc_nodes\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "    @property\n",
    "    def clip_lstm_grads(self):\n",
    "        return self._clip_lstm_grads\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self._learning_rate\n",
    "    @property\n",
    "    def num_word_threshold(self):\n",
    "        return self._num_word_threshold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "hps = HParams(\n",
    "        num_embedding_size = 16,\n",
    "        num_timesteps = 50,\n",
    "        num_lstm_nodes = [32, 32],\n",
    "        num_lstm_layers = 2,\n",
    "        num_fc_nodes = 32,\n",
    "        batch_size = 100,\n",
    "        clip_lstm_grads = 1.0,\n",
    "        learning_rate = 0.001,\n",
    "        # 字符出现次数最小值\n",
    "        num_word_threshold = 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_file = 'cnews_data/cnews.train.seg.txt'\n",
    "val_file = 'cnews_data/cnews.val.seg.txt'\n",
    "test_file = 'cnews_data/cnews.test.seg.txt'\n",
    "vocab_file = 'cnews_data/cnews.vocab.txt'\n",
    "category_file = 'cnews_data/cnews.category.txt'\n",
    "output_folder = 'cnews_data/run_text_rnn'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 77331\n",
      "INFO:tensorflow:category_size: 10\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "\n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r', encoding = 'utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    def sentence_to_id(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "\n",
    "class CategoryDict:\n",
    "    def __init__(self, file_name):\n",
    "        self._category_to_id = {}\n",
    "        with open(file_name, 'r', encoding = 'utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Exception(\"%s is not in our category list\" % category)\n",
    "        return self._category_to_id[category]\n",
    "\n",
    "vocab = Vocab(vocab_file, hps.num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "tf.logging.info('vocab_size: %d' % vocab_size)\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "tf.logging.info('category_size: %d' % num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # matrix\n",
    "        self._inputs = []\n",
    "        # vector\n",
    "        self._outputs = []\n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "\n",
    "    def _parse_file(self,filename):\n",
    "        tf.logging.info(\"Loading data from %s\", filename)\n",
    "        with open(filename, 'r', encoding = 'utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label)\n",
    "            id_words = self._vocab.sentence_to_id(content)\n",
    "            id_words = id_words[0:self._num_timesteps]\n",
    "            padding_num = self._num_timesteps - len(id_words)\n",
    "            id_words = id_words + \\\n",
    "                       [self._vocab.unk for i in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "        self._inputs = np.asarray(self._inputs, dtype = np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype = np.int32)\n",
    "        self._random_shuffle()\n",
    "        self._num_examples = len(self._inputs)\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Exception(\"batch_size: %d is too large\" % batch_size)\n",
    "\n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_outputs = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_outputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading data from cnews_data/cnews.train.seg.txt\n",
      "INFO:tensorflow:Loading data from cnews_data/cnews.val.seg.txt\n",
      "INFO:tensorflow:Loading data from cnews_data/cnews.test.seg.txt\n",
      "(array([[ 2173,   162,  6076,  7020,    37,     0,   374,    17,   250,\n",
      "           16,   976,   149,     2,    30, 46755,     1,    18,     4,\n",
      "        28096, 10342, 23911,     2,     0,  1465,     1,  1291,    30,\n",
      "         2134,     2, 70262,  5633,     3,    60,  5633,  4753,   181,\n",
      "            0,     2,     1,    19,  6758,  1069,    18,   126, 41083,\n",
      "            3,  4595,    19,   449,    18],\n",
      "       [ 9801,   128,   502,   433,   650,    61,   685,   197,    17,\n",
      "          250,    16, 10815,  9801,   267,  1624,  7134,   157,  2385,\n",
      "          385,     1,  3523,  9801,   926,  4936,     0,   377,    84,\n",
      "            1,  2239,     1,   544, 21729,    12,  1330,   173,    37,\n",
      "           77, 13692,     6,  4828,     3,   267,   459,   730,  4574,\n",
      "          358,   220,   375,     1, 28281]]), array([4, 4]))\n",
      "(array([[  467,    11, 22188, 11410, 15117,   190,  1050,  1371,   843,\n",
      "         1804,   476,  2320,    11, 22188, 11410, 15117,     1,   721,\n",
      "           46,   838,  3872,  4460,  6577, 11410, 15198,   456,     3,\n",
      "          172,   190,  1050,    24,    42,  3320, 14248,    92,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0],\n",
      "       [26578,  2491, 53988,     0,    15, 43190,    14, 23329,    17,\n",
      "          250,    16,  3389,  4771,    17,    16,  1897,    50,    88,\n",
      "           22,     4, 28726,     2,  4946,  1310,    15, 43190,    14,\n",
      "           21,  1223,     8,     0,     9,     1,    31,   329, 15905,\n",
      "        41093,    53,  1441,  7012,    50, 24110,    92,    71,    15,\n",
      "        28313,    14,  1228,   587,   579]]), array([5, 1]))\n",
      "(array([[ 1751, 16784,  1840,  5103,  2359,  1647,    58,     4,  3828,\n",
      "         5956,    46,   186,   175,   723,     0, 18436,    72,    23,\n",
      "          180,   814,    17,    76,     0,    16,  2626,   732,    26,\n",
      "         3751,   202,    13,    67,    91,    50,   315,    13,   189,\n",
      "         7963,    31, 11857,   110, 18566,  4525,   124,     1,    12,\n",
      "          315,   143,   189,  2421,  1484],\n",
      "       [ 1173,   320,   905,    52,     0, 13418,  3375, 24220,   320,\n",
      "         1314,  5613,  1550,    17,  1647,    16,   100,    23,   863,\n",
      "          814,   160,    71,    15, 26002,    14,   270,     1,    71,\n",
      "            2,   320,    17,  1647,    16,   905,    52,  1476, 13418,\n",
      "        27823,     1,   197,  3375,   320,  1082,  1147,    47,  1569,\n",
      "          970,   232,    13,   469,     1]]), array([0, 4]))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataSet(\n",
    "    train_file, vocab, category_vocab, hps.num_timesteps\n",
    ")\n",
    "val_dataset = TextDataSet(\n",
    "    val_file, vocab, category_vocab, hps.num_timesteps\n",
    ")\n",
    "test_dataset = TextDataSet(\n",
    "    test_file, vocab, category_vocab, hps.num_timesteps\n",
    ")\n",
    "print(train_dataset.next_batch(2))\n",
    "print(val_dataset.next_batch(2))\n",
    "print(test_dataset.next_batch(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def create_model(hps, vocab_size, num_classes):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "\n",
    "    inputs = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    outputs = tf.placeholder(tf.int32, (batch_size, ))\n",
    "    # dropout 保存下来的值\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    # 保存模型训练到哪一步\n",
    "\n",
    "    global_step = tf.Variable(\n",
    "        tf.zeros([], tf.int64), name = 'global_step', trainable = False\n",
    "    )\n",
    "    # 随机的均匀分布中初始化\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer = embedding_initializer):\n",
    "         embeddings = tf.get_variable(\n",
    "             'embeddinggg',\n",
    "             [vocab_size, hps.num_embedding_size],\n",
    "             tf.float32\n",
    "         )\n",
    "         # 对应inputs & embeddings 拼接成矩阵\n",
    "         # [1, 10, 7] -> [embeddings[1], embeddings[10], embeddings[7]]\n",
    "         embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "\n",
    "    def _generate_params_for_lstm_cell(x_size, h_size, bias_size):\n",
    "        \"\"\"\n",
    "        generates parameters for pure lstm implementation.\n",
    "        :param X_size: 输入大小\n",
    "        :param h_size: 上一步输出大小\n",
    "        :param bias_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x_w = tf.get_variable('x_weights', x_size)\n",
    "        h_w = tf.get_variable('h_weights', h_size)\n",
    "        b = tf.get_variable('biases', bias_size,\n",
    "                            initializer = tf.constant_initializer(0.0))\n",
    "        return x_w, h_w, b\n",
    "\n",
    "    with tf.variable_scope('lstm_nn', initializer = lstm_init):\n",
    "        \"\"\"\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell(\n",
    "                hps.num_lstm_nodes[i],\n",
    "                state_is_tuple = True\n",
    "            )\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                cell,\n",
    "                output_keep_prob = keep_prob\n",
    "            )\n",
    "            cells.append(cell)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        #rnn_outputs: [batch_size, num_timesteps, lstm_outputs[-1]]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(\n",
    "            cell,embed_inputs,initial_state = initial_state)\n",
    "        last = rnn_outputs[:,-1,:]\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('inputs'):\n",
    "            ix, ih, ib = _generate_params_for_lstm_cell(\n",
    "                x_size = [hps.num_embedding_size, hps.num_lstm_nodes[0]],\n",
    "                h_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size = [1, hps.num_lstm_nodes[0]]\n",
    "            )\n",
    "        with tf.variable_scope('outputs'):\n",
    "            ox, oh, ob = _generate_params_for_lstm_cell(\n",
    "                x_size = [hps.num_embedding_size, hps.num_lstm_nodes[0]],\n",
    "                h_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size = [1, hps.num_lstm_nodes[0]]\n",
    "            )\n",
    "        with tf.variable_scope('forget'):\n",
    "            fx, fh, fb = _generate_params_for_lstm_cell(\n",
    "                x_size = [hps.num_embedding_size, hps.num_lstm_nodes[0]],\n",
    "                h_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size = [1, hps.num_lstm_nodes[0]]\n",
    "            )\n",
    "        with tf.variable_scope('memory'):\n",
    "            cx, ch, cb = _generate_params_for_lstm_cell(\n",
    "                x_size = [hps.num_embedding_size, hps.num_lstm_nodes[0]],\n",
    "                h_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size = [1, hps.num_lstm_nodes[0]]\n",
    "            )\n",
    "        state = tf.Variable(\n",
    "            tf.zeros([batch_size, hps.num_lstm_nodes[0]]),\n",
    "            trainable = False\n",
    "        )\n",
    "        h = tf.Variable(\n",
    "            tf.zeros([batch_size, hps.num_lstm_nodes[0]]),\n",
    "            trainable = False\n",
    "        )\n",
    "\n",
    "        for i in range(num_timesteps):\n",
    "            # [batch_size, 1, embed_size]\n",
    "            embed_input = embed_inputs[:,i,:]\n",
    "            embed_input = tf.reshape(embed_input,\n",
    "                                      [batch_size, hps.num_embedding_size])\n",
    "            forget_gate = tf.sigmoid(\n",
    "                tf.matmul(embed_input, fx) + tf.matmul(h, fh) + fb)\n",
    "            input_gate = tf.sigmoid(\n",
    "                tf.matmul(embed_input, ix) + tf.matmul(h, ih) + ib)\n",
    "            output_gate = tf.sigmoid(\n",
    "                tf.matmul(embed_input, ox) + tf.matmul(h, oh) + ob)\n",
    "            mid_state = tf.tanh(\n",
    "                tf.matmul(embed_input, cx) + tf.matmul(h, ch) + cb)\n",
    "            state = mid_state * input_gate + state * forget_gate\n",
    "            h = output_gate * tf.tanh(state)\n",
    "        last = h\n",
    "\n",
    "\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor = 1.0)\n",
    "    with tf.variable_scope('fc', initializer = fc_init):\n",
    "        fc1 = tf.layers.dense(last,\n",
    "                              hps.num_fc_nodes,\n",
    "                              activation = tf.nn.relu,\n",
    "                              name = 'fc1')\n",
    "        fc1_dropout = tf.layers.dropout(fc1, keep_prob)\n",
    "        # 映射到类别\n",
    "        logits = tf.layers.dense(fc1_dropout, num_classes, name = 'fc2')\n",
    "\n",
    "    with tf.name_scope('metrics'):\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits = logits, labels = outputs\n",
    "        )\n",
    "        loss = tf.reduce_mean(softmax_loss)\n",
    "        # [0,2,5,4,2] -> argmax: 2 在2的维度上最大\n",
    "        y_pred = tf.argmax(tf.nn.softmax(logits),\n",
    "                           1,\n",
    "                           output_type = tf.int32)\n",
    "        correct_pred = tf.equal(outputs, y_pred)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    with tf.name_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info('variable name %s' % var.name)\n",
    "        # 限制梯度大小\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                                             global_step = global_step)\n",
    "\n",
    "        return ((inputs, outputs, keep_prob),\n",
    "                (loss, accuracy),\n",
    "                (train_op, global_step))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\zyc\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:507: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-15-8dea79a6256b>:117: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\zyc\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-8dea79a6256b>:121: dropout (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "INFO:tensorflow:variable name embedding/embeddinggg:0\n",
      "INFO:tensorflow:variable name lstm_nn/inputs/x_weights:0\n",
      "INFO:tensorflow:variable name lstm_nn/inputs/h_weights:0\n",
      "INFO:tensorflow:variable name lstm_nn/inputs/biases:0\n",
      "INFO:tensorflow:variable name lstm_nn/outputs/x_weights:0\n",
      "INFO:tensorflow:variable name lstm_nn/outputs/h_weights:0\n",
      "INFO:tensorflow:variable name lstm_nn/outputs/biases:0\n",
      "INFO:tensorflow:variable name lstm_nn/forget/x_weights:0\n",
      "INFO:tensorflow:variable name lstm_nn/forget/h_weights:0\n",
      "INFO:tensorflow:variable name lstm_nn/forget/biases:0\n",
      "INFO:tensorflow:variable name lstm_nn/memory/x_weights:0\n",
      "INFO:tensorflow:variable name lstm_nn/memory/h_weights:0\n",
      "INFO:tensorflow:variable name lstm_nn/memory/biases:0\n",
      "INFO:tensorflow:variable name fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name fc/fc2/kernel:0\n",
      "INFO:tensorflow:variable name fc/fc2/bias:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "placeholders, metrics, others = create_model(\n",
    "    hps, vocab_size, num_classes)\n",
    "\n",
    "inputs, outputs, keep_prob = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def eval_holdout(sess, accuracy, dataset_for_test, batch_size):\n",
    "    num_batches = dataset_for_test.num_examples() // batch_size\n",
    "    tf.logging.info(\"Eval holdout: num_examples = %d, batch_size = %d\",\n",
    "                    dataset_for_test.num_examples(), batch_size)\n",
    "    accuracy_vals = []\n",
    "    for i in range(num_batches):\n",
    "        batch_inputs, batch_labels = dataset_for_test.next_batch(batch_size)\n",
    "        accuracy_val = sess.run(accuracy,\n",
    "                                feed_dict = {\n",
    "                                    inputs: batch_inputs,\n",
    "                                    outputs: batch_labels,\n",
    "                                    keep_prob: 1.0,\n",
    "                                })\n",
    "        accuracy_vals.append(accuracy_val)\n",
    "    return np.mean(accuracy_vals)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:   200, loss: 2.072, accuracy: 0.190\n",
      "INFO:tensorflow:Step:   400, loss: 1.534, accuracy: 0.360\n",
      "INFO:tensorflow:Step:   600, loss: 1.503, accuracy: 0.450\n",
      "INFO:tensorflow:Step:   800, loss: 1.227, accuracy: 0.510\n",
      "INFO:tensorflow:Step:  1000, loss: 1.074, accuracy: 0.610\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  1000, val_accuracy: 0.505, test_accuracy: 0.544\n",
      "INFO:tensorflow:Step:  1200, loss: 0.813, accuracy: 0.670\n",
      "INFO:tensorflow:Step:  1400, loss: 0.818, accuracy: 0.690\n",
      "INFO:tensorflow:Step:  1600, loss: 0.868, accuracy: 0.720\n",
      "INFO:tensorflow:Step:  1800, loss: 0.809, accuracy: 0.800\n",
      "INFO:tensorflow:Step:  2000, loss: 0.596, accuracy: 0.770\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  2000, val_accuracy: 0.666, test_accuracy: 0.696\n",
      "INFO:tensorflow:Step:  2200, loss: 0.606, accuracy: 0.790\n",
      "INFO:tensorflow:Step:  2400, loss: 0.429, accuracy: 0.860\n",
      "INFO:tensorflow:Step:  2600, loss: 0.304, accuracy: 0.880\n",
      "INFO:tensorflow:Step:  2800, loss: 0.419, accuracy: 0.860\n",
      "INFO:tensorflow:Step:  3000, loss: 0.282, accuracy: 0.930\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  3000, val_accuracy: 0.771, test_accuracy: 0.785\n",
      "INFO:tensorflow:Step:  3200, loss: 0.331, accuracy: 0.880\n",
      "INFO:tensorflow:Step:  3400, loss: 0.413, accuracy: 0.910\n",
      "INFO:tensorflow:Step:  3600, loss: 0.322, accuracy: 0.890\n",
      "INFO:tensorflow:Step:  3800, loss: 0.192, accuracy: 0.930\n",
      "INFO:tensorflow:Step:  4000, loss: 0.190, accuracy: 0.940\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  4000, val_accuracy: 0.811, test_accuracy: 0.833\n",
      "INFO:tensorflow:Step:  4200, loss: 0.346, accuracy: 0.920\n",
      "INFO:tensorflow:Step:  4400, loss: 0.132, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  4600, loss: 0.066, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  4800, loss: 0.184, accuracy: 0.940\n",
      "INFO:tensorflow:Step:  5000, loss: 0.049, accuracy: 0.980\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  5000, val_accuracy: 0.836, test_accuracy: 0.847\n",
      "INFO:tensorflow:Step:  5200, loss: 0.139, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  5400, loss: 0.184, accuracy: 0.950\n",
      "INFO:tensorflow:Step:  5600, loss: 0.173, accuracy: 0.950\n",
      "INFO:tensorflow:Step:  5800, loss: 0.289, accuracy: 0.920\n",
      "INFO:tensorflow:Step:  6000, loss: 0.187, accuracy: 0.960\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  6000, val_accuracy: 0.827, test_accuracy: 0.853\n",
      "INFO:tensorflow:Step:  6200, loss: 0.167, accuracy: 0.960\n",
      "INFO:tensorflow:Step:  6400, loss: 0.100, accuracy: 0.960\n",
      "INFO:tensorflow:Step:  6600, loss: 0.059, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  6800, loss: 0.019, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  7000, loss: 0.055, accuracy: 0.980\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  7000, val_accuracy: 0.834, test_accuracy: 0.855\n",
      "INFO:tensorflow:Step:  7200, loss: 0.028, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  7400, loss: 0.074, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  7600, loss: 0.015, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  7800, loss: 0.088, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  8000, loss: 0.012, accuracy: 1.000\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  8000, val_accuracy: 0.837, test_accuracy: 0.857\n",
      "INFO:tensorflow:Step:  8200, loss: 0.006, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  8400, loss: 0.128, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  8600, loss: 0.035, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  8800, loss: 0.022, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  9000, loss: 0.010, accuracy: 0.990\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  9000, val_accuracy: 0.839, test_accuracy: 0.859\n",
      "INFO:tensorflow:Step:  9200, loss: 0.066, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  9400, loss: 0.002, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  9600, loss: 0.004, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  9800, loss: 0.016, accuracy: 0.990\n",
      "INFO:tensorflow:Step: 10000, loss: 0.003, accuracy: 1.000\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step: 10000, val_accuracy: 0.847, test_accuracy: 0.866\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.8\n",
    "test_keeo_prob_value = 1.0\n",
    "\n",
    "num_train_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs,batch_labels = train_dataset.next_batch(hps.batch_size)\n",
    "        outputs_val = sess.run([loss, accuracy,train_op, global_step],\n",
    "                           feed_dict = {\n",
    "                               inputs: batch_inputs,\n",
    "                               outputs: batch_labels,\n",
    "                               keep_prob: train_keep_prob_value\n",
    "                           })\n",
    "        loss_val, accuracy_val, _,global_step_val = outputs_val\n",
    "        if global_step_val % 200 == 0:\n",
    "            tf.logging.info(\"Step: %5d, loss: %3.3f, accuracy: %3.3f\"\n",
    "                            % (global_step_val, loss_val, accuracy_val))\n",
    "        if global_step_val % 1000 == 0:\n",
    "            accuracy_eval = eval_holdout(sess, accuracy, val_dataset, hps.batch_size)\n",
    "            accuracy_test = eval_holdout(sess, accuracy, test_dataset, hps.batch_size)\n",
    "            tf.logging.info(\"Step: %5d, val_accuracy: %3.3f, test_accuracy: %3.3f\"\n",
    "                            % (global_step_val, accuracy_eval, accuracy_test))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-71e1ac8c",
   "language": "python",
   "display_name": "PyCharm (MachineLearning)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}