{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\zyc\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\"\"\"\n",
    "1.Data generator\n",
    "    a. Loads vocab\n",
    "    b. Loads image features\n",
    "    c. provide data for training\n",
    "2. Builds image caption model.\n",
    "3. Trains the model.\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow.io.gfile as gfile\n",
    "from tensorflow.compat.v1 import logging\n",
    "import pprint\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "input_description_file = \"./image_caption_data/results_20130124.token\"\n",
    "input_img_feature_file = \"./image_caption_data/feature_extraction_inception_v3\"\n",
    "input_vocab_file = \"./image_caption_data/vocab.txt\"\n",
    "output_dir = \"./image_caption_data/local_run\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "class HParams:\n",
    "    def __init__(self,\n",
    "                 num_vocab_word_threshold,\n",
    "                 num_embedding_nodes,\n",
    "                 num_timesteps,\n",
    "                 num_lstm_nodes,\n",
    "                 num_lstm_layers,\n",
    "                 num_fc_nodes,\n",
    "                 batch_size,\n",
    "                 cell_type,\n",
    "                 clip_lstm_grades,\n",
    "                 learning_rate,\n",
    "                 keep_prob,\n",
    "                 log_frequent,\n",
    "                 save_frequent):\n",
    "        self.num_vocab_word_threshold = num_vocab_word_threshold\n",
    "        self.num_embedding_nodes = num_embedding_nodes\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.num_lstm_nodes = num_lstm_nodes\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.num_fc_nodes = num_fc_nodes\n",
    "        self.batch_size = batch_size\n",
    "        self.cell_type = cell_type\n",
    "        self.clip_lstm_grades = clip_lstm_grades\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.log_frequent = log_frequent\n",
    "        self.save_frequent = save_frequent\n",
    "\n",
    "def get_default_params():\n",
    "    return HParams(\n",
    "        num_vocab_word_threshold = 3,\n",
    "        num_embedding_nodes = 32,\n",
    "        num_timesteps = 10,\n",
    "        num_lstm_nodes = [64,64],\n",
    "        num_lstm_layers = 2,\n",
    "        num_fc_nodes = 32,\n",
    "        batch_size = 80,\n",
    "        cell_type = \"lstm\",\n",
    "        clip_lstm_grades = 1.0,\n",
    "        learning_rate = 0.001,\n",
    "        keep_prob = 0.8,\n",
    "        log_frequent = 10,\n",
    "        save_frequent = 100\n",
    "    )\n",
    "\n",
    "hps = get_default_params()\n",
    "print(hps.save_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        self._id_to_word = {}\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "\n",
    "    def _read_dict(self, filename):\n",
    "        with gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, occurrence = line.strip('\\r\\n').split('\\t')\n",
    "            occurrence = int(occurrence)\n",
    "            if occurrence < self._word_num_threshold:\n",
    "                continue\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            if word in self._word_to_id or idx in self._id_to_word:\n",
    "                raise Exception(\"duplicate words in vocab.\")\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self.unk)\n",
    "    def id_to_word(self, word):\n",
    "        return self._id_to_word.get(word, '<UNK>')\n",
    "    def size(self):\n",
    "        return len(self._id_to_word)\n",
    "    def encode(self, sentence):\n",
    "        return [self.word_to_id(word) for word in sentence.split(' ')]\n",
    "    def decode(self, sentence_id):\n",
    "        words = [self.id_to_word(word_id) for word_id in sentence_id]\n",
    "        return ' '.join(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 10875\n",
      "[1494, 389, 1, 0]\n",
      "'the of man white'\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(input_vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "logging.info(\"vocab_size: %d\" % vocab_size)\n",
    "pprint.pprint(vocab.encode(\"I have a dream.\"))\n",
    "pprint.pprint(vocab.decode([5, 10, 9, 20]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def parse_token_file(token_file):\n",
    "    \"\"\"Parses image description file.\"\"\"\n",
    "    img_name_to_tokens = {}\n",
    "    with gfile.GFile(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            img_id, description = line.strip('\\r\\n').split('\\t')\n",
    "            img_name, _ = img_id.split('#')\n",
    "            img_name_to_tokens.setdefault(img_name, [])\n",
    "            img_name_to_tokens[img_name].append(description)\n",
    "    return img_name_to_tokens\n",
    "\n",
    "def convert_token_to_id(img_name_to_tokens, vocab):\n",
    "    \"\"\"Converts tokens of each description of imgs to id.\"\"\"\n",
    "    img_name_to_tokens_id = {}\n",
    "    for img_name in img_name_to_tokens:\n",
    "        img_name_to_tokens_id.setdefault(img_name, [])\n",
    "        for description in img_name_to_tokens[img_name]:\n",
    "            token_ids = vocab.encode(description)\n",
    "            img_name_to_tokens_id[img_name].append(token_ids)\n",
    "    return img_name_to_tokens_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "img_name_to_tokens = parse_token_file(input_description_file)\n",
    "img_name_to_tokens_id = convert_token_to_id(img_name_to_tokens, vocab)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:num of all imgs: 31783\n",
      "['A man in jeans is reclining on a green metal bench along a busy sidewalk and '\n",
      " 'crowded street .',\n",
      " 'A white male with a blue sweater and gray pants laying on a sidewalk bench .',\n",
      " 'A man in a blue shirt and gray pants is sleeping on a sidewalk bench .',\n",
      " 'A person is sleeping on a bench , next to cars .',\n",
      " 'A man sleeping on a bench in a city area .']\n",
      "INFO:tensorflow:num if all imgs: 31783\n",
      "[[3, 9, 4, 132, 8, 3532, 6, 1, 48, 337, 146, 139, 1, 244, 93, 7, 380, 36, 2],\n",
      " [3, 20, 179, 11, 1, 26, 284, 7, 120, 128, 297, 6, 1, 93, 146, 2],\n",
      " [3, 9, 4, 1, 26, 21, 7, 120, 128, 8, 340, 6, 1, 93, 146, 2],\n",
      " [3, 63, 8, 340, 6, 1, 146, 12, 70, 15, 518, 2],\n",
      " [3, 9, 340, 6, 1, 146, 4, 1, 112, 171, 2]]\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"num of all imgs: %d\" % len(img_name_to_tokens))\n",
    "pprint.pprint(img_name_to_tokens['2778832101.jpg'])\n",
    "logging.info(\"num if all imgs: %d\" % len(img_name_to_tokens_id))\n",
    "pprint.pprint(img_name_to_tokens_id['2778832101.jpg'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class ImageCaptionData(object):\n",
    "    \"\"\"Provides data for image caption model.\"\"\"\n",
    "    def __init__(self,\n",
    "                 image_name_to_token_id,\n",
    "                 image_feature_dir,\n",
    "                 num_timesteps,\n",
    "                 vocab,\n",
    "                 deterministic = False):\n",
    "        self._vocab = vocab\n",
    "        self._image_name_to_token_id = image_name_to_token_id\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._deterministic = deterministic\n",
    "        self._indicator = 0\n",
    "\n",
    "        self._img_feature_filenames = []\n",
    "        self._img_feature_data = []\n",
    "\n",
    "        self._all_img_feature_filepaths = []\n",
    "        for filename in gfile.listdir(image_feature_dir):\n",
    "            self._all_img_feature_filepaths.append(\n",
    "                os.path.join(image_feature_dir,filename)\n",
    "            )\n",
    "        pprint.pprint(self._all_img_feature_filepaths)\n",
    "        self._load_img_feature_pickle()\n",
    "\n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "\n",
    "    def _load_img_feature_pickle(self):\n",
    "        \"\"\"Load img feature data from pickle\"\"\"\n",
    "        for filepath in self._all_img_feature_filepaths:\n",
    "            logging.info(\"loading %s\" % filepath)\n",
    "            with gfile.GFile(filepath, 'rb') as  f:\n",
    "                filenames, features = cPickle.load(f,encoding = 'iso-8859-1')\n",
    "                self._img_feature_filenames += filenames\n",
    "                self._img_feature_data.append(features)\n",
    "        # 按照最后一个维度合并\n",
    "        self._img_feature_data = np.vstack(self._img_feature_data)\n",
    "        origin_shape = self._img_feature_data.shape\n",
    "        # 去掉中间两个维度\n",
    "        self._img_feature_data = np.reshape(\n",
    "            self._img_feature_data,\n",
    "            (origin_shape[0], origin_shape[3])\n",
    "        )\n",
    "        self._img_feature_filenames = np.asarray(self._img_feature_filenames)\n",
    "        print(self._img_feature_data.shape)\n",
    "        print(self._img_feature_filenames.shape)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._img_feature_filenames)\n",
    "\n",
    "    def img_feature_size(self):\n",
    "        return self._img_feature_data.shape[1]\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        \"\"\"Shuffle data randomly.\"\"\"\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._img_feature_filenames = self._img_feature_filenames[p]\n",
    "        self._img_feature_data = self._img_feature_data[p]\n",
    "\n",
    "    def img_desc(self, batch_filenames):\n",
    "        \"\"\"Gets descriptions for filenames in batch.\"\"\"\n",
    "        batch_sentence_ids = []\n",
    "        batch_wights = []\n",
    "        for filename in batch_filenames:\n",
    "            token_ids_set = self._image_name_to_token_id[filename]\n",
    "            chosen_token_ids = random.choice(token_ids_set)\n",
    "            chosen_token_ids_length = len(chosen_token_ids)\n",
    "\n",
    "            weight = [1 for i in range(chosen_token_ids_length)]\n",
    "            if chosen_token_ids_length >= self._num_timesteps:\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:\n",
    "                remaining_length = self._num_timesteps - chosen_token_ids_length\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)]\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_wights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_wights = np.asarray(batch_wights)\n",
    "        return batch_sentence_ids, batch_wights\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Returns next batch data.\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.size():\n",
    "            if not self._deterministic:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator < self.size()\n",
    "\n",
    "        batch_filenames = self._img_feature_filenames[self._indicator: end_indicator]\n",
    "        batch_img_features = self._img_feature_data[self._indicator: end_indicator]\n",
    "        # sentence_ids: [100,101,0,0,0] -> [1,1,0,0,0]\n",
    "        # batch_weights: 计算权重,不计算<UNK>的损失函数\n",
    "        batch_sentence_ids, batch_weights = self.img_desc(batch_filenames)\n",
    "        self._indicator = end_indicator\n",
    "        return batch_img_features, batch_sentence_ids, batch_weights, batch_filenames"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./image_caption_data/feature_extraction_inception_v3\\\\image_features-0.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-1.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-10.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-11.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-12.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-13.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-14.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-15.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-16.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-17.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-18.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-19.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-2.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-20.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-21.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-22.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-23.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-24.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-25.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-26.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-27.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-28.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-29.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-3.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-30.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-31.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-4.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-5.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-6.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-7.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-8.pickle',\n",
      " './image_caption_data/feature_extraction_inception_v3\\\\image_features-9.pickle']\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-0.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-1.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-10.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-11.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-12.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-13.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-14.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-15.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-16.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-17.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-18.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-19.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-2.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-20.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-21.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-22.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-23.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-24.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-25.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-26.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-27.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-28.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-29.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-3.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-30.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-31.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-4.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-5.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-6.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-7.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-8.pickle\n",
      "INFO:tensorflow:loading ./image_caption_data/feature_extraction_inception_v3\\image_features-9.pickle\n",
      "(31783, 2048)\n",
      "(31783,)\n",
      "INFO:tensorflow:img_features_dim: 2048\n",
      "INFO:tensorflow:caption_data_size: 31783\n",
      "array([[0.16847758, 0.03755009, 0.09539981, ..., 0.17809397, 0.2873377 ,\n",
      "        0.6882727 ],\n",
      "       [0.73604053, 0.20823029, 0.5555779 , ..., 1.3068527 , 0.16931126,\n",
      "        0.16628894],\n",
      "       [0.29562825, 0.4582695 , 0.27044564, ..., 0.38819027, 0.21867152,\n",
      "        0.00780819],\n",
      "       [0.56320256, 0.547667  , 0.11056374, ..., 0.45646876, 0.13788392,\n",
      "        0.18161753],\n",
      "       [0.30797467, 0.23121092, 0.16420001, ..., 0.3102528 , 0.00470707,\n",
      "        0.60597664]], dtype=float32)\n",
      "array([[  59,   53,   14,   35,  415,   12,    5,   13,    4,   26],\n",
      "       [ 984,   30, 1483,  874,   11,   40, 2241,    2,    2,    2],\n",
      "       [   3,  282,  272,   33,   18,    1, 2568,   77,    4,   48],\n",
      "       [ 313,  106,   17,    5,  449,   10,    1,   57,  794,    4],\n",
      "       [1925,  111,    4,   22,  209, 2311,    1,  111,    4,    1]])\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "array(['1875237930.jpg', '4232753024.jpg', '3559781965.jpg',\n",
      "       '2623486983.jpg', '8052380671.jpg'], dtype='<U14')\n"
     ]
    }
   ],
   "source": [
    "caption_data = ImageCaptionData(img_name_to_tokens_id,\n",
    "                                input_img_feature_file,\n",
    "                                hps.num_timesteps,\n",
    "                                vocab)\n",
    "\n",
    "img_feature_dim = caption_data.img_feature_size()\n",
    "caption_data_size = caption_data.size()\n",
    "logging.info(\"img_features_dim: %d\" % img_feature_dim)\n",
    "logging.info(\"caption_data_size: %d\" % caption_data_size)\n",
    "\n",
    "batch_img_features, batch_sentence_ids, batch_weights, batch_img_names = caption_data.next_batch(5)\n",
    "pprint.pprint(batch_img_features)\n",
    "pprint.pprint(batch_sentence_ids)\n",
    "pprint.pprint(batch_weights)\n",
    "pprint.pprint(batch_img_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\zyc\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:507: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-9-7107778b5b86>:56: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\zyc\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-7107778b5b86>:4: BasicLSTMCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-7107778b5b86>:71: MultiRNNCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-7107778b5b86>:74: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\zyc\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:738: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\zyc\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:744: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-9-7107778b5b86>:85: dropout (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "INFO:tensorflow:variable name: embeddings/embedding:0\n",
      "INFO:tensorflow:variable name: embeddings/img_feature_embed/dense/kernel:0\n",
      "INFO:tensorflow:variable name: embeddings/img_feature_embed/dense/bias:0\n",
      "INFO:tensorflow:variable name: embeddings/lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: embeddings/lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: embeddings/lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: embeddings/lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: embeddings/fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: embeddings/fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: embeddings/fc/logits/kernel:0\n",
      "INFO:tensorflow:variable name: embeddings/fc/logits/bias:0\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    \"\"\"Returns specific cell according to cell_type\"\"\"\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.nn.rnn_cell.BasicLSTMCell(hidden_dim,\n",
    "                                            state_is_tuple = True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.nn.rnn_cell.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s type has not been supported.\" % cell_type)\n",
    "\n",
    "def dropout(cell, keep_prob):\n",
    "    \"\"\"Wrap cell with dropout.\"\"\"\n",
    "    return tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = keep_prob)\n",
    "\n",
    "def get_train_model(hps, vocab_size, img_feature_dim):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "\n",
    "    img_feature = tf.placeholder(tf.float32,\n",
    "                                 (batch_size, img_feature_dim))\n",
    "    sentence = tf.placeholder(tf.int32,\n",
    "                              (batch_size, num_timesteps))\n",
    "    mask = tf.placeholder(tf.int32,\n",
    "                          (batch_size, num_timesteps))\n",
    "    keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")\n",
    "    global_step = tf.Variable(tf.zeros([], tf.int32),\n",
    "                              name = \"global_step\",\n",
    "                              trainable = False)\n",
    "    # prediction process:\n",
    "    # sentence: [a, b, c, d, e]\n",
    "    # input: [image, a, b, c, d]\n",
    "    # image_feature: [0.4, 0.3, 10, 2]\n",
    "    # predict #1: image_feature -> embedding_img -> lstm -> (a)\n",
    "    # predict #2: a -> embedding_word -> lstm -> (b)\n",
    "    # predict #3: b -> embedding_word -> lstm -> (c)\n",
    "    # ....\n",
    "\n",
    "    # Sets up embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embeddings',\n",
    "                           initializer = embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embedding',\n",
    "            [vocab_size, hps.num_embedding_nodes],\n",
    "            tf.float32)\n",
    "        # embed_token_ids: [batch_size, num_timesteps - 1, num_embedding_nodes]\n",
    "        embed_token_ids = tf.nn.embedding_lookup(\n",
    "            embeddings,\n",
    "            sentence[:,0: num_timesteps - 1])\n",
    "        img_feature_embed_init = tf.uniform_unit_scaling_initializer(\n",
    "            factor = 1.0)\n",
    "        with tf.variable_scope('img_feature_embed',\n",
    "                               initializer = img_feature_embed_init):\n",
    "            # img_feature: [batch_size, img_feature_dim]\n",
    "            # embed_img: [batch_size, num_embedding_nodes]\n",
    "            embed_img = tf.layers.dense(img_feature, hps.num_embedding_nodes)\n",
    "            # embed_img: [batch_size, 1, num_embedding_nodes]\n",
    "            embed_img = tf.expand_dims(embed_img, 1)\n",
    "            #embed_inputs: [batch_size, num_timesteps, num_embedding_nodes]\n",
    "            embed_inputs = tf.concat([embed_img, embed_token_ids], axis = 1)\n",
    "\n",
    "        # Sets up rnn network\n",
    "        scale = 1.0 / math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "        rnn_init = tf.random_uniform_initializer(-scale, scale)\n",
    "        with tf.variable_scope('lstm_nn', initializer = rnn_init):\n",
    "            cells = []\n",
    "            for i in range(hps.num_lstm_layers):\n",
    "                cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "                cell = dropout(cell, keep_prob)\n",
    "                cells.append(cell)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "            init_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "            # rnn_outputs: [batch_size, num_timestep, hps.num_lstm_node[-1]]\n",
    "            rnn_outputs, _ = tf.nn.dynamic_rnn(cell,\n",
    "                                               embed_inputs,\n",
    "                                               initial_state = init_state)\n",
    "        # Sets up fully-connected layer\n",
    "        fc_init = tf.uniform_unit_scaling_initializer(factor = 1.0)\n",
    "        with tf.variable_scope('fc', initializer = fc_init):\n",
    "            rnn_outputs_2d = tf.reshape(rnn_outputs,\n",
    "                                        [-1, hps.num_lstm_nodes[-1]])\n",
    "            fc1  = tf.layers.dense(rnn_outputs_2d,\n",
    "                                  hps.num_fc_nodes,\n",
    "                                  name = \"fc1\")\n",
    "            fc1_dropout = tf.layers.dropout(fc1, keep_prob)\n",
    "            fc1_relu = tf.nn.relu(fc1_dropout)\n",
    "            logits = tf.layers.dense(fc1_relu,\n",
    "                                     vocab_size,\n",
    "                                     name = \"logits\")\n",
    "\n",
    "        # Calculates loss\n",
    "        with tf.variable_scope('loss'):\n",
    "            sentence_flatten = tf.reshape(sentence, [-1])\n",
    "            mask_flatten = tf.reshape(mask, [-1])\n",
    "            mask_sum = tf.reduce_sum(mask_flatten)\n",
    "            mask_flatten = tf.cast(mask_flatten, tf.float32)\n",
    "            mask_sum = tf.cast(mask_sum, tf.float32)\n",
    "\n",
    "            softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits = logits,\n",
    "                labels = sentence_flatten)\n",
    "\n",
    "            weighted_softmax_loss = tf.multiply(\n",
    "                softmax_loss, tf.cast(mask_flatten, tf.float32))\n",
    "            loss = tf.reduce_sum(weighted_softmax_loss) / tf.cast(mask_sum, tf.float32)\n",
    "\n",
    "            prediction = tf.argmax(logits, 1, output_type = tf.int32)\n",
    "            correct_prediction = tf.equal(prediction,\n",
    "                                          sentence_flatten)\n",
    "            weighted_correct_prediction = tf.multiply(\n",
    "                tf.cast(correct_prediction, tf.float32),\n",
    "                tf.cast(mask_flatten, tf.float32))\n",
    "            accuracy = tf.reduce_sum(weighted_correct_prediction) / mask_sum\n",
    "            tf.summary.scalar('loss', loss)\n",
    "\n",
    "        # Defines train op.\n",
    "        with tf.variable_scope('train_op'):\n",
    "            tvars = tf.trainable_variables()\n",
    "            for var in tvars:\n",
    "                logging.info('variable name: %s' % var.name)\n",
    "                grads, _ = tf.clip_by_global_norm(\n",
    "                    tf.gradients(loss, tvars), hps.clip_lstm_grades)\n",
    "                optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "                train_op = optimizer.apply_gradients(\n",
    "                    zip(grads, tvars),global_step = global_step)\n",
    "\n",
    "        return ((img_feature, sentence, mask, keep_prob),\n",
    "                (loss, accuracy, train_op),\n",
    "                global_step)\n",
    "\n",
    "place_holders, metrics, global_step = get_train_model(hps, vocab_size, img_feature_dim)\n",
    "img_feature, sentence, mask, keep_prob = place_holders\n",
    "loss, accuracy, train_op = metrics\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep = 10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:    10, loss: 9.133, accu: 0.092\n",
      "INFO:tensorflow:Step:    20, loss: 8.538, accu: 0.085\n",
      "INFO:tensorflow:Step:    30, loss: 7.778, accu: 0.091\n",
      "INFO:tensorflow:Step:    40, loss: 7.243, accu: 0.094\n",
      "INFO:tensorflow:Step:    50, loss: 6.757, accu: 0.090\n",
      "INFO:tensorflow:Step:    60, loss: 6.243, accu: 0.137\n",
      "INFO:tensorflow:Step:    70, loss: 6.078, accu: 0.141\n",
      "INFO:tensorflow:Step:    80, loss: 6.124, accu: 0.153\n",
      "INFO:tensorflow:Step:    90, loss: 5.750, accu: 0.146\n",
      "INFO:tensorflow:Step:   100, loss: 5.644, accu: 0.151\n",
      "INFO:tensorflow:Step:   100, model saved\n",
      "INFO:tensorflow:Step:   110, loss: 5.780, accu: 0.132\n",
      "INFO:tensorflow:Step:   120, loss: 5.523, accu: 0.155\n",
      "INFO:tensorflow:Step:   130, loss: 5.527, accu: 0.144\n",
      "INFO:tensorflow:Step:   140, loss: 5.524, accu: 0.165\n",
      "INFO:tensorflow:Step:   150, loss: 5.564, accu: 0.157\n",
      "INFO:tensorflow:Step:   160, loss: 5.269, accu: 0.173\n",
      "INFO:tensorflow:Step:   170, loss: 5.493, accu: 0.151\n",
      "INFO:tensorflow:Step:   180, loss: 5.540, accu: 0.161\n",
      "INFO:tensorflow:Step:   190, loss: 5.464, accu: 0.170\n",
      "INFO:tensorflow:Step:   200, loss: 5.461, accu: 0.163\n",
      "INFO:tensorflow:Step:   200, model saved\n",
      "INFO:tensorflow:Step:   210, loss: 5.416, accu: 0.173\n",
      "INFO:tensorflow:Step:   220, loss: 5.460, accu: 0.175\n",
      "INFO:tensorflow:Step:   230, loss: 5.379, accu: 0.173\n",
      "INFO:tensorflow:Step:   240, loss: 5.327, accu: 0.180\n",
      "INFO:tensorflow:Step:   250, loss: 5.315, accu: 0.174\n",
      "INFO:tensorflow:Step:   260, loss: 5.275, accu: 0.181\n",
      "INFO:tensorflow:Step:   270, loss: 5.157, accu: 0.175\n",
      "INFO:tensorflow:Step:   280, loss: 5.373, accu: 0.188\n",
      "INFO:tensorflow:Step:   290, loss: 5.171, accu: 0.208\n",
      "INFO:tensorflow:Step:   300, loss: 5.151, accu: 0.209\n",
      "INFO:tensorflow:Step:   300, model saved\n",
      "INFO:tensorflow:Step:   310, loss: 5.250, accu: 0.194\n",
      "INFO:tensorflow:Step:   320, loss: 5.318, accu: 0.181\n",
      "INFO:tensorflow:Step:   330, loss: 5.199, accu: 0.169\n",
      "INFO:tensorflow:Step:   340, loss: 5.132, accu: 0.186\n",
      "INFO:tensorflow:Step:   350, loss: 5.109, accu: 0.196\n",
      "INFO:tensorflow:Step:   360, loss: 5.081, accu: 0.206\n",
      "INFO:tensorflow:Step:   370, loss: 5.253, accu: 0.199\n",
      "INFO:tensorflow:Step:   380, loss: 5.125, accu: 0.196\n",
      "INFO:tensorflow:Step:   390, loss: 5.118, accu: 0.194\n",
      "INFO:tensorflow:Step:   400, loss: 4.958, accu: 0.208\n",
      "INFO:tensorflow:Step:   400, model saved\n",
      "INFO:tensorflow:Step:   410, loss: 4.993, accu: 0.237\n",
      "INFO:tensorflow:Step:   420, loss: 5.126, accu: 0.224\n",
      "INFO:tensorflow:Step:   430, loss: 4.897, accu: 0.233\n",
      "INFO:tensorflow:Step:   440, loss: 5.054, accu: 0.208\n",
      "INFO:tensorflow:Step:   450, loss: 4.942, accu: 0.224\n",
      "INFO:tensorflow:Step:   460, loss: 5.087, accu: 0.206\n",
      "INFO:tensorflow:Step:   470, loss: 4.970, accu: 0.219\n",
      "INFO:tensorflow:Step:   480, loss: 4.913, accu: 0.219\n",
      "INFO:tensorflow:Step:   490, loss: 5.066, accu: 0.205\n",
      "INFO:tensorflow:Step:   500, loss: 5.010, accu: 0.227\n",
      "INFO:tensorflow:Step:   500, model saved\n",
      "INFO:tensorflow:Step:   510, loss: 4.936, accu: 0.217\n",
      "INFO:tensorflow:Step:   520, loss: 4.933, accu: 0.243\n",
      "INFO:tensorflow:Step:   530, loss: 4.949, accu: 0.209\n",
      "INFO:tensorflow:Step:   540, loss: 4.921, accu: 0.238\n",
      "INFO:tensorflow:Step:   550, loss: 5.090, accu: 0.218\n",
      "INFO:tensorflow:Step:   560, loss: 4.702, accu: 0.261\n",
      "INFO:tensorflow:Step:   570, loss: 4.915, accu: 0.247\n",
      "INFO:tensorflow:Step:   580, loss: 4.641, accu: 0.239\n",
      "INFO:tensorflow:Step:   590, loss: 4.528, accu: 0.260\n",
      "INFO:tensorflow:Step:   600, loss: 4.565, accu: 0.272\n",
      "INFO:tensorflow:Step:   600, model saved\n",
      "INFO:tensorflow:Step:   610, loss: 4.568, accu: 0.259\n",
      "INFO:tensorflow:Step:   620, loss: 4.852, accu: 0.242\n",
      "INFO:tensorflow:Step:   630, loss: 4.793, accu: 0.246\n",
      "INFO:tensorflow:Step:   640, loss: 4.729, accu: 0.248\n",
      "INFO:tensorflow:Step:   650, loss: 4.768, accu: 0.255\n",
      "INFO:tensorflow:Step:   660, loss: 4.668, accu: 0.249\n",
      "INFO:tensorflow:Step:   670, loss: 4.796, accu: 0.227\n",
      "INFO:tensorflow:Step:   680, loss: 4.647, accu: 0.251\n",
      "INFO:tensorflow:Step:   690, loss: 4.749, accu: 0.262\n",
      "INFO:tensorflow:Step:   700, loss: 4.799, accu: 0.241\n",
      "INFO:tensorflow:Step:   700, model saved\n",
      "INFO:tensorflow:Step:   710, loss: 4.515, accu: 0.270\n",
      "INFO:tensorflow:Step:   720, loss: 4.658, accu: 0.258\n",
      "INFO:tensorflow:Step:   730, loss: 4.610, accu: 0.264\n",
      "INFO:tensorflow:Step:   740, loss: 4.720, accu: 0.246\n",
      "INFO:tensorflow:Step:   750, loss: 4.485, accu: 0.252\n",
      "INFO:tensorflow:Step:   760, loss: 4.649, accu: 0.250\n",
      "INFO:tensorflow:Step:   770, loss: 4.618, accu: 0.274\n",
      "INFO:tensorflow:Step:   780, loss: 4.476, accu: 0.252\n",
      "INFO:tensorflow:Step:   790, loss: 4.507, accu: 0.263\n",
      "INFO:tensorflow:Step:   800, loss: 4.788, accu: 0.231\n",
      "INFO:tensorflow:Step:   800, model saved\n",
      "INFO:tensorflow:Step:   810, loss: 4.560, accu: 0.258\n",
      "INFO:tensorflow:Step:   820, loss: 4.385, accu: 0.275\n",
      "INFO:tensorflow:Step:   830, loss: 4.515, accu: 0.247\n",
      "INFO:tensorflow:Step:   840, loss: 4.425, accu: 0.272\n",
      "INFO:tensorflow:Step:   850, loss: 4.572, accu: 0.248\n",
      "INFO:tensorflow:Step:   860, loss: 4.727, accu: 0.267\n",
      "INFO:tensorflow:Step:   870, loss: 4.709, accu: 0.238\n",
      "INFO:tensorflow:Step:   880, loss: 4.538, accu: 0.293\n",
      "INFO:tensorflow:Step:   890, loss: 4.488, accu: 0.282\n",
      "INFO:tensorflow:Step:   900, loss: 4.434, accu: 0.258\n",
      "INFO:tensorflow:Step:   900, model saved\n",
      "INFO:tensorflow:Step:   910, loss: 4.613, accu: 0.248\n",
      "INFO:tensorflow:Step:   920, loss: 4.666, accu: 0.246\n",
      "INFO:tensorflow:Step:   930, loss: 4.539, accu: 0.251\n",
      "INFO:tensorflow:Step:   940, loss: 4.442, accu: 0.267\n",
      "INFO:tensorflow:Step:   950, loss: 4.344, accu: 0.278\n",
      "INFO:tensorflow:Step:   960, loss: 4.492, accu: 0.282\n",
      "INFO:tensorflow:Step:   970, loss: 4.423, accu: 0.272\n",
      "INFO:tensorflow:Step:   980, loss: 4.506, accu: 0.275\n",
      "INFO:tensorflow:Step:   990, loss: 4.410, accu: 0.251\n",
      "INFO:tensorflow:Step:  1000, loss: 4.501, accu: 0.263\n",
      "INFO:tensorflow:Step:  1000, model saved\n"
     ]
    }
   ],
   "source": [
    "training_steps = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    writer = tf.summary.FileWriter(output_dir, sess.graph)\n",
    "    for i in range(training_steps):\n",
    "        (batch_img_features,\n",
    "         batch_sentence_ids,\n",
    "         batch_weights, _) = caption_data.next_batch(hps.batch_size)\n",
    "        input_vals = (batch_img_features,\n",
    "                      batch_sentence_ids,\n",
    "                      batch_weights,\n",
    "                      hps.keep_prob)\n",
    "        feed_dict = dict(zip(place_holders, input_vals))\n",
    "        fetches = [global_step, loss, accuracy, train_op]\n",
    "        should_log = (i + 1) % hps.log_frequent == 0\n",
    "        should_save = (i + 1) % hps.save_frequent == 0\n",
    "        if should_log:\n",
    "            fetches += [summary_op]\n",
    "\n",
    "        outputs = sess.run(fetches, feed_dict = feed_dict)\n",
    "        global_step_val, loss_val, accuracy_val = outputs[0:3]\n",
    "        if should_log:\n",
    "            summary_str = outputs[-1]\n",
    "            writer.add_summary(summary_str, global_step_val)\n",
    "            logging.info('Step: %5d, loss: %3.3f, accu: %3.3f'\n",
    "                         % (global_step_val, loss_val, accuracy_val))\n",
    "        if should_save:\n",
    "            model_save_file = os.path.join(output_dir, \"image_caption\")\n",
    "            logging.info('Step: %5d, model saved' % global_step_val)\n",
    "            saver.save(sess, model_save_file, global_step = global_step_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-71e1ac8c",
   "language": "python",
   "display_name": "PyCharm (MachineLearning)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}