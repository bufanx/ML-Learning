{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\zyc\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "class VGGNet:\n",
    "    \"\"\"\n",
    "    Builds VGG_16 net structure,\n",
    "    load parameters from pre-train models\n",
    "    \"\"\"\n",
    "    def __init__(self,data_dict):\n",
    "        self.data_dict = data_dict\n",
    "\n",
    "    # 获取卷积层权重\n",
    "    def get_conv_filters(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name = 'conv')\n",
    "\n",
    "    # 获取全连接层权重\n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name = 'fc')\n",
    "\n",
    "    # 获取偏置\n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name = 'bias')\n",
    "\n",
    "    # 创建卷积层\n",
    "    def conv_layer(self, X, name):\n",
    "        \"\"\"Builds convolution layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self.get_conv_filters(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            # [1,1,1,1] 是步长\n",
    "            h = tf.nn.conv2d(X, conv_w, [1, 1, 1, 1], padding = 'SAME')\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "\n",
    "    def pooling_layer(self, X, name):\n",
    "        \"\"\"Builds pooling layer.\"\"\"\n",
    "        return tf.nn.max_pool(X,\n",
    "                              ksize = [1,2,2,1],\n",
    "                              strides = [1,2,2,1],\n",
    "                              padding = 'SAME',\n",
    "                              name = name)\n",
    "\n",
    "    def fc_layer(self, X, name, activation = tf.nn.relu):\n",
    "        \"\"\"Builds full-connected layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(X, fc_w)\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation is None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "\n",
    "    def flatten_layer(self, X, name):\n",
    "        # 展平操作.\n",
    "        with tf.name_scope(name):\n",
    "            # [batch_size, image_width, image_height, channel]\n",
    "            X_shape = X.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in X_shape[1:]:\n",
    "                dim *= d\n",
    "            X = tf.reshape(X, [-1, dim])\n",
    "            return X\n",
    "    def build(self, X_rgb):\n",
    "        \"\"\"\n",
    "        Build VGG16 network structure.\n",
    "        :param X_rgb:[1,224,224,3]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print('building model...')\n",
    "        r,g,b = tf.split(X_rgb, [1,1,1], axis = 3)\n",
    "        X_bgr = tf.concat(\n",
    "            [b - VGG_MEAN[0],\n",
    "             g - VGG_MEAN[1],\n",
    "             r - VGG_MEAN[2]],\n",
    "             axis = 3)\n",
    "        assert X_bgr.get_shape().as_list()[1:] == [224,224,3]\n",
    "\n",
    "        self.conv1_1 = self.conv_layer(X_bgr,'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1,'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2,'pool1')\n",
    "\n",
    "        self.conv2_1 = self.conv_layer(self.pool1,'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1,'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2,'pool2')\n",
    "\n",
    "        self.conv3_1 = self.conv_layer(self.pool2,'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1,'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2,'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3,'pool3')\n",
    "\n",
    "        self.conv4_1 = self.conv_layer(self.pool3,'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1,'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2,'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3,'pool4')\n",
    "\n",
    "        self.conv5_1 = self.conv_layer(self.pool4,'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1,'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2,'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3,'pool5')\n",
    "\n",
    "        '''\n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "        self.fc8 = self.fc_layer(self.fc7, 'fc8',activation = None)\n",
    "        self.prob = tf.nn.softmax(self.fc8, name = 'prob')\n",
    "        '''\n",
    "\n",
    "        print('building model finished: %4ds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "vgg16_npy_path = './vgg16.npy'\n",
    "content_img_path = './gugong.jpg'\n",
    "style_img_path = './xingkong.jpeg'\n",
    "\n",
    "num_steps = 100\n",
    "learning_rate = 10\n",
    "\n",
    "lambda_c = 0.1\n",
    "lambda_s = 500\n",
    "\n",
    "output_dir = './run_style_transfer'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "# data_dict = np.load(vgg16_npy_path, allow_pickle=True, encoding = 'latin1').item()\n",
    "#\n",
    "# vgg16_for_result = VGGNet(data_dict)\n",
    "# content = tf.placeholder(tf.float32, shape = [1, 224, 224, 3])\n",
    "# vgg16_for_result.build(content)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model...\n",
      "building model finished:    0s\n",
      "building model...\n",
      "building model finished:    0s\n",
      "building model...\n",
      "building model finished:    0s\n"
     ]
    }
   ],
   "source": [
    "def initial_result(shape, mean, stddev):\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def read_img(img_name):\n",
    "    img = Image.open(img_name)\n",
    "    np_img = np.array(img) #(224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype=np.int32) #(1, 224, 224, 3)\n",
    "    return np_img\n",
    "\n",
    "def gram_matrix(X):\n",
    "    \"\"\"\n",
    "    Calculates gram matrix\n",
    "    :param X:从某一个卷积层得到的一个输出.shape: [1, width, height, channel]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    b, w, h, ch = X.get_shape().as_list()\n",
    "    features = tf.reshape(X, [b, h * w, ch]) # [ch, ch] -> (i, j)\n",
    "    # [h*w, ch] matrix -> [ch, h*w] * [h*w, ch] -> [ch, ch]\n",
    "    gram = tf.matmul(features, features, adjoint_a = True)\\\n",
    "        / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "\n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "content_value = read_img(content_img_path)\n",
    "style_value = read_img(style_img_path)\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "style = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "\n",
    "data_dict = np.load(vgg16_npy_path, allow_pickle=True, encoding = 'latin1').item()\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2\n",
    "    #vgg_for_content.conv2_2,\n",
    "    # vgg_for_content.conv3_3,\n",
    "    # vgg_for_content.conv4_3,\n",
    "    # vgg_for_content.conv5_3\n",
    "]\n",
    "\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2\n",
    "    #vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    # vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "\n",
    "# feature_size: [1, width, height, channel]\n",
    "style_features = [\n",
    "    # vgg_for_style.conv1_2,\n",
    "    # vgg_for_style.conv2_2,\n",
    "    # vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3\n",
    "    # vgg_for_style.conv5_3\n",
    "]\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "result_style_features = [\n",
    "    #vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    vgg_for_result.conv4_3\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# shape: [1, width, height, channel]\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1, 2, 3])\n",
    "\n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1, 2])\n",
    "\n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 14148.2148, content_loss: 60539.4297, style_loss:  16.1885\n",
      "step: 2, loss_value: 11805.9795, content_loss: 46199.5273, style_loss:  14.3721\n",
      "step: 3, loss_value: 9158.5498, content_loss: 37584.0000, style_loss:  10.8003\n",
      "step: 4, loss_value: 7825.1973, content_loss: 33188.3555, style_loss:   9.0127\n",
      "step: 5, loss_value: 6730.3184, content_loss: 30543.1406, style_loss:   7.3520\n",
      "step: 6, loss_value: 5946.3809, content_loss: 28898.6250, style_loss:   6.1130\n",
      "step: 7, loss_value: 5145.8096, content_loss: 27882.9941, style_loss:   4.7150\n",
      "step: 8, loss_value: 4986.7275, content_loss: 27275.9746, style_loss:   4.5183\n",
      "step: 9, loss_value: 4660.3511, content_loss: 26864.5527, style_loss:   3.9478\n",
      "step: 10, loss_value: 4423.4785, content_loss: 26601.3691, style_loss:   3.5267\n",
      "step: 11, loss_value: 4358.4941, content_loss: 26369.4023, style_loss:   3.4431\n",
      "step: 12, loss_value: 4114.0415, content_loss: 26015.6113, style_loss:   3.0250\n",
      "step: 13, loss_value: 4032.7256, content_loss: 25571.6523, style_loss:   2.9511\n",
      "step: 14, loss_value: 3860.8843, content_loss: 25047.5566, style_loss:   2.7123\n",
      "step: 15, loss_value: 3743.4561, content_loss: 24394.8477, style_loss:   2.6079\n",
      "step: 16, loss_value: 3589.7993, content_loss: 23614.6973, style_loss:   2.4567\n",
      "step: 17, loss_value: 3439.9165, content_loss: 22783.2969, style_loss:   2.3232\n",
      "step: 18, loss_value: 3317.9268, content_loss: 21908.0469, style_loss:   2.2542\n",
      "step: 19, loss_value: 3174.2974, content_loss: 20974.9863, style_loss:   2.1536\n",
      "step: 20, loss_value: 3029.9272, content_loss: 20054.9395, style_loss:   2.0489\n",
      "step: 21, loss_value: 2898.2256, content_loss: 19130.7773, style_loss:   1.9703\n",
      "step: 22, loss_value: 2781.6216, content_loss: 18208.5410, style_loss:   1.9215\n",
      "step: 23, loss_value: 2652.1367, content_loss: 17347.0684, style_loss:   1.8349\n",
      "step: 24, loss_value: 2536.3906, content_loss: 16506.1406, style_loss:   1.7716\n",
      "step: 25, loss_value: 2419.6426, content_loss: 15739.5371, style_loss:   1.6914\n",
      "step: 26, loss_value: 2321.4639, content_loss: 15008.2529, style_loss:   1.6413\n",
      "step: 27, loss_value: 2228.6631, content_loss: 14351.3408, style_loss:   1.5871\n",
      "step: 28, loss_value: 2165.1753, content_loss: 13731.1318, style_loss:   1.5841\n",
      "step: 29, loss_value: 2149.0037, content_loss: 13218.1182, style_loss:   1.6544\n",
      "step: 30, loss_value: 2352.9866, content_loss: 12711.0801, style_loss:   2.1638\n",
      "step: 31, loss_value: 1997.8877, content_loss: 12376.7422, style_loss:   1.5204\n",
      "step: 32, loss_value: 1975.6353, content_loss: 12047.1201, style_loss:   1.5418\n",
      "step: 33, loss_value: 2009.5027, content_loss: 11717.7695, style_loss:   1.6755\n",
      "step: 34, loss_value: 1842.2241, content_loss: 11474.8945, style_loss:   1.3895\n",
      "step: 35, loss_value: 1886.9841, content_loss: 11256.5664, style_loss:   1.5227\n",
      "step: 36, loss_value: 1874.1228, content_loss: 11003.2959, style_loss:   1.5476\n",
      "step: 37, loss_value: 1728.5525, content_loss: 10816.5430, style_loss:   1.2938\n",
      "step: 38, loss_value: 1798.1901, content_loss: 10643.2344, style_loss:   1.4677\n",
      "step: 39, loss_value: 1739.5385, content_loss: 10405.9512, style_loss:   1.3979\n",
      "step: 40, loss_value: 1652.4780, content_loss: 10217.8359, style_loss:   1.2614\n",
      "step: 41, loss_value: 1691.6304, content_loss: 10065.0176, style_loss:   1.3703\n",
      "step: 42, loss_value: 1620.4902, content_loss: 9856.5859, style_loss:   1.2697\n",
      "step: 43, loss_value: 1568.0790, content_loss: 9687.9365, style_loss:   1.1986\n",
      "step: 44, loss_value: 1577.1582, content_loss: 9538.7432, style_loss:   1.2466\n",
      "step: 45, loss_value: 1528.2012, content_loss: 9340.8076, style_loss:   1.1882\n",
      "step: 46, loss_value: 1485.0161, content_loss: 9187.0879, style_loss:   1.1326\n",
      "step: 47, loss_value: 1480.1372, content_loss: 9045.6953, style_loss:   1.1511\n",
      "step: 48, loss_value: 1450.7727, content_loss: 8855.1992, style_loss:   1.1305\n",
      "step: 49, loss_value: 1425.6677, content_loss: 8708.6572, style_loss:   1.1096\n",
      "step: 50, loss_value: 1398.6858, content_loss: 8564.6123, style_loss:   1.0844\n",
      "step: 51, loss_value: 1359.5283, content_loss: 8409.3369, style_loss:   1.0372\n",
      "step: 52, loss_value: 1344.9213, content_loss: 8256.9873, style_loss:   1.0384\n",
      "step: 53, loss_value: 1325.2075, content_loss: 8110.7827, style_loss:   1.0283\n",
      "step: 54, loss_value: 1325.8682, content_loss: 7984.2856, style_loss:   1.0549\n",
      "step: 55, loss_value: 1386.7645, content_loss: 7825.7896, style_loss:   1.2084\n",
      "step: 56, loss_value: 1460.2864, content_loss: 7754.3579, style_loss:   1.3697\n",
      "step: 57, loss_value: 1610.9563, content_loss: 7621.8228, style_loss:   1.6975\n",
      "step: 58, loss_value: 1307.9001, content_loss: 7616.1060, style_loss:   1.0926\n",
      "step: 59, loss_value: 1422.5148, content_loss: 7625.3433, style_loss:   1.3200\n",
      "step: 60, loss_value: 1398.7953, content_loss: 7589.3745, style_loss:   1.2797\n",
      "step: 61, loss_value: 1337.9034, content_loss: 7619.3091, style_loss:   1.1519\n",
      "step: 62, loss_value: 1379.2021, content_loss: 7664.3999, style_loss:   1.2255\n",
      "step: 63, loss_value: 1341.8120, content_loss: 7638.9688, style_loss:   1.1558\n",
      "step: 64, loss_value: 1302.8347, content_loss: 7636.6748, style_loss:   1.0783\n",
      "step: 65, loss_value: 1300.3489, content_loss: 7642.6733, style_loss:   1.0722\n",
      "step: 66, loss_value: 1271.4462, content_loss: 7587.9482, style_loss:   1.0253\n",
      "step: 67, loss_value: 1263.5762, content_loss: 7515.4565, style_loss:   1.0241\n",
      "step: 68, loss_value: 1234.1143, content_loss: 7442.1211, style_loss:   0.9798\n",
      "step: 69, loss_value: 1220.4995, content_loss: 7348.9858, style_loss:   0.9712\n",
      "step: 70, loss_value: 1198.6909, content_loss: 7247.8877, style_loss:   0.9478\n",
      "step: 71, loss_value: 1187.6600, content_loss: 7153.4409, style_loss:   0.9446\n",
      "step: 72, loss_value: 1158.1119, content_loss: 7036.7642, style_loss:   0.9089\n",
      "step: 73, loss_value: 1151.4121, content_loss: 6906.2920, style_loss:   0.9216\n",
      "step: 74, loss_value: 1131.1266, content_loss: 6789.5742, style_loss:   0.9043\n",
      "step: 75, loss_value: 1118.3168, content_loss: 6683.9360, style_loss:   0.8998\n",
      "step: 76, loss_value: 1102.2900, content_loss: 6588.7354, style_loss:   0.8868\n",
      "step: 77, loss_value: 1094.1589, content_loss: 6487.1494, style_loss:   0.8909\n",
      "step: 78, loss_value: 1098.5293, content_loss: 6410.1187, style_loss:   0.9150\n",
      "step: 79, loss_value: 1215.7469, content_loss: 6295.6318, style_loss:   1.1724\n",
      "step: 80, loss_value: 1413.9744, content_loss: 6287.3188, style_loss:   1.5705\n",
      "step: 81, loss_value: 1602.7739, content_loss: 6226.7021, style_loss:   1.9602\n",
      "step: 82, loss_value: 1249.1042, content_loss: 6371.1367, style_loss:   1.2240\n",
      "step: 83, loss_value: 1465.8518, content_loss: 6632.9146, style_loss:   1.6051\n",
      "step: 84, loss_value: 1311.8516, content_loss: 6856.5728, style_loss:   1.2524\n",
      "step: 85, loss_value: 1338.6569, content_loss: 7138.7422, style_loss:   1.2496\n",
      "step: 86, loss_value: 1322.7798, content_loss: 7429.8125, style_loss:   1.1596\n",
      "step: 87, loss_value: 1304.3416, content_loss: 7647.0479, style_loss:   1.0793\n",
      "step: 88, loss_value: 1311.9639, content_loss: 7805.0811, style_loss:   1.0629\n",
      "step: 89, loss_value: 1296.4919, content_loss: 7916.0122, style_loss:   1.0098\n",
      "step: 90, loss_value: 1283.0118, content_loss: 7948.8848, style_loss:   0.9762\n",
      "step: 91, loss_value: 1264.1357, content_loss: 7901.1328, style_loss:   0.9480\n",
      "step: 92, loss_value: 1237.6132, content_loss: 7797.7598, style_loss:   0.9157\n",
      "step: 93, loss_value: 1213.3129, content_loss: 7651.7056, style_loss:   0.8963\n",
      "step: 94, loss_value: 1189.3524, content_loss: 7486.3477, style_loss:   0.8814\n",
      "step: 95, loss_value: 1164.6075, content_loss: 7305.7974, style_loss:   0.8681\n",
      "step: 96, loss_value: 1142.7104, content_loss: 7110.0791, style_loss:   0.8634\n",
      "step: 97, loss_value: 1116.4954, content_loss: 6923.4453, style_loss:   0.8483\n",
      "step: 98, loss_value: 1094.3638, content_loss: 6733.1411, style_loss:   0.8421\n",
      "step: 99, loss_value: 1075.0978, content_loss: 6556.0542, style_loss:   0.8390\n",
      "step: 100, loss_value: 1056.6724, content_loss: 6385.4253, style_loss:   0.8363\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        loss_value, content_loss_value, style_loss_value, _ = \\\n",
    "            sess.run([loss, content_loss, style_loss, train_op],\n",
    "                     feed_dict = {\n",
    "                        content: content_value,\n",
    "                         style: style_value\n",
    "                     })\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' \\\n",
    "              % (step + 1,\n",
    "                 loss_value[0],\n",
    "                 content_loss_value[0],\n",
    "                 style_loss_value[0]))\n",
    "        result_img_path = os.path.join(\n",
    "            output_dir, 'result-%05d.jpg' % (step + 1)\n",
    "        )\n",
    "        result_val = result.eval(sess)[0]\n",
    "        result_val = np.clip(result_val, 0, 255)\n",
    "        img_arr = np.asarray(result_val, np.uint8)\n",
    "        img = Image.fromarray(img_arr)\n",
    "        img.save(result_img_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-71e1ac8c",
   "language": "python",
   "display_name": "PyCharm (MachineLearning)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}