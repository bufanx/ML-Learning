{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "class VGGNet:\n",
    "    \"\"\"\n",
    "    Builds VGG_16 net structure,\n",
    "    load parameters from pre-train models\n",
    "    \"\"\"\n",
    "    def __init__(self,data_dict):\n",
    "        self.data_dict = data_dict\n",
    "\n",
    "    # 获取卷积层权重\n",
    "    def get_conv_filters(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name = 'conv')\n",
    "\n",
    "    # 获取全连接层权重\n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name = 'fc')\n",
    "\n",
    "    # 获取偏置\n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name = 'bias')\n",
    "\n",
    "    # 创建卷积层\n",
    "    def conv_layer(self, X, name):\n",
    "        \"\"\"Builds convolution layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self.get_conv_filters(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            # [1,1,1,1] 是步长\n",
    "            h = tf.nn.conv2d(X, conv_w, [1, 1, 1, 1], padding = 'SAME')\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "\n",
    "    def pooling_layer(self, X, name):\n",
    "        \"\"\"Builds pooling layer.\"\"\"\n",
    "        return tf.nn.max_pool(X,\n",
    "                              ksize = [1,2,2,1],\n",
    "                              strides = [1,2,2,1],\n",
    "                              padding = 'SAME',\n",
    "                              name = name)\n",
    "\n",
    "    def fc_layer(self, X, name, activation = tf.nn.relu):\n",
    "        \"\"\"Builds full-connected layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(X, fc_w)\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation is None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "\n",
    "    def flatten_layer(self, X, name):\n",
    "        # 展平操作.\n",
    "        with tf.name_scope(name):\n",
    "            # [batch_size, image_width, image_height, channel]\n",
    "            X_shape = X.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in X_shape[1:]:\n",
    "                dim *= d\n",
    "            X = tf.reshape(X, [-1, dim])\n",
    "            return X\n",
    "    def build(self, X_rgb):\n",
    "        \"\"\"\n",
    "        Build VGG16 network structure.\n",
    "        :param X_rgb:[1,224,224,3]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print('building model...')\n",
    "        r,g,b = tf.split(X_rgb, [1,1,1], axis = 3)\n",
    "        X_bgr = tf.concat(\n",
    "            [b - VGG_MEAN[0],\n",
    "             g - VGG_MEAN[1],\n",
    "             r - VGG_MEAN[2]],\n",
    "             axis = 3)\n",
    "        assert X_bgr.get_shape().as_list()[1:] == [224,224,3]\n",
    "\n",
    "        self.conv1_1 = self.conv_layer(X_bgr,'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1,'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2,'pool1')\n",
    "\n",
    "        self.conv2_1 = self.conv_layer(self.pool1,'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1,'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2,'pool2')\n",
    "\n",
    "        self.conv3_1 = self.conv_layer(self.pool2,'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1,'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2,'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3,'pool3')\n",
    "\n",
    "        self.conv4_1 = self.conv_layer(self.pool3,'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1,'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2,'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3,'pool4')\n",
    "\n",
    "        self.conv5_1 = self.conv_layer(self.pool4,'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1,'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2,'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3,'pool5')\n",
    "\n",
    "        '''\n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "        self.fc8 = self.fc_layer(self.fc7, 'fc8',activation = None)\n",
    "        self.prob = tf.nn.softmax(self.fc8, name = 'prob')\n",
    "        '''\n",
    "\n",
    "        print('building model finished: %4ds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "vgg16_npy_path = './vgg16.npy'\n",
    "content_img_path = './gugong.jpg'\n",
    "style_img_path = './xingkong.jpeg'\n",
    "\n",
    "num_steps = 100\n",
    "learning_rate = 10\n",
    "\n",
    "lambda_c = 0.1\n",
    "lambda_s = 500\n",
    "\n",
    "output_dir = './run_style_transfer'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "# data_dict = np.load(vgg16_npy_path, allow_pickle=True, encoding = 'latin1').item()\n",
    "#\n",
    "# vgg16_for_result = VGGNet(data_dict)\n",
    "# content = tf.placeholder(tf.float32, shape = [1, 224, 224, 3])\n",
    "# vgg16_for_result.build(content)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model...\n",
      "building model finished:    0s\n",
      "building model...\n",
      "building model finished:    0s\n",
      "building model...\n",
      "building model finished:    0s\n"
     ]
    }
   ],
   "source": [
    "def initial_result(shape, mean, stddev):\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def read_img(img_name):\n",
    "    img = Image.open(img_name)\n",
    "    np_img = np.array(img) #(224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype=np.int32) #(1, 224, 224, 3)\n",
    "    return np_img\n",
    "\n",
    "def gram_matrix(X):\n",
    "    \"\"\"\n",
    "    Calculates gram matrix\n",
    "    :param X:从某一个卷积层得到的一个输出.shape: [1, width, height, channel]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    b, w, h, ch = X.get_shape().as_list()\n",
    "    features = tf.reshape(X, [b, h * w, ch]) # [ch, ch] -> (i, j)\n",
    "    # [h*w, ch] matrix -> [ch, h*w] * [h*w, ch] -> [ch, ch]\n",
    "    gram = tf.matmul(features, features, adjoint_a = True)\\\n",
    "        / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "\n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "content_value = read_img(content_img_path)\n",
    "style_value = read_img(style_img_path)\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "style = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "\n",
    "data_dict = np.load(vgg16_npy_path, allow_pickle=True, encoding = 'latin1').item()\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2\n",
    "    #vgg_for_content.conv2_2,\n",
    "    # vgg_for_content.conv3_3,\n",
    "    # vgg_for_content.conv4_3,\n",
    "    # vgg_for_content.conv5_3\n",
    "]\n",
    "\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2\n",
    "    #vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    # vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "\n",
    "# feature_size: [1, width, height, channel]\n",
    "style_features = [\n",
    "    # vgg_for_style.conv1_2,\n",
    "    # vgg_for_style.conv2_2,\n",
    "    # vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3\n",
    "    # vgg_for_style.conv5_3\n",
    "]\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "result_style_features = [\n",
    "    #vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    vgg_for_result.conv4_3\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# shape: [1, width, height, channel]\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1, 2, 3])\n",
    "\n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1, 2])\n",
    "\n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 14143.6895, content_loss: 60400.4961, style_loss:  16.2073\n",
      "step: 2, loss_value: 11810.3896, content_loss: 45856.9297, style_loss:  14.4494\n",
      "step: 3, loss_value: 9081.0078, content_loss: 37219.8984, style_loss:  10.7180\n",
      "step: 4, loss_value: 7218.0146, content_loss: 32550.7109, style_loss:   7.9259\n",
      "step: 5, loss_value: 6784.0840, content_loss: 29948.1270, style_loss:   7.5785\n",
      "step: 6, loss_value: 6321.9780, content_loss: 28788.4941, style_loss:   6.8863\n",
      "step: 7, loss_value: 5476.2290, content_loss: 28222.9180, style_loss:   5.3079\n",
      "step: 8, loss_value: 5110.4473, content_loss: 28100.3164, style_loss:   4.6008\n",
      "step: 9, loss_value: 4787.3984, content_loss: 28063.9922, style_loss:   3.9620\n",
      "step: 10, loss_value: 4633.9863, content_loss: 28036.0254, style_loss:   3.6608\n",
      "step: 11, loss_value: 4410.9878, content_loss: 27950.5781, style_loss:   3.2319\n",
      "step: 12, loss_value: 4257.6484, content_loss: 27759.0684, style_loss:   2.9635\n",
      "step: 13, loss_value: 4133.4409, content_loss: 27411.5996, style_loss:   2.7846\n",
      "step: 14, loss_value: 3992.9500, content_loss: 26936.7676, style_loss:   2.5985\n",
      "step: 15, loss_value: 3846.6108, content_loss: 26364.8477, style_loss:   2.4203\n",
      "step: 16, loss_value: 3713.8213, content_loss: 25681.1191, style_loss:   2.2914\n",
      "step: 17, loss_value: 3554.0583, content_loss: 24865.8555, style_loss:   2.1349\n",
      "step: 18, loss_value: 3420.0710, content_loss: 23967.7266, style_loss:   2.0466\n",
      "step: 19, loss_value: 3281.2122, content_loss: 23029.2402, style_loss:   1.9566\n",
      "step: 20, loss_value: 3142.9136, content_loss: 22060.4102, style_loss:   1.8737\n",
      "step: 21, loss_value: 3006.2415, content_loss: 21081.1230, style_loss:   1.7963\n",
      "step: 22, loss_value: 2876.9685, content_loss: 20114.5488, style_loss:   1.7310\n",
      "step: 23, loss_value: 2745.1938, content_loss: 19154.0938, style_loss:   1.6596\n",
      "step: 24, loss_value: 2624.9749, content_loss: 18234.1934, style_loss:   1.6031\n",
      "step: 25, loss_value: 2514.5854, content_loss: 17362.5156, style_loss:   1.5567\n",
      "step: 26, loss_value: 2412.3728, content_loss: 16538.6680, style_loss:   1.5170\n",
      "step: 27, loss_value: 2316.2036, content_loss: 15787.3379, style_loss:   1.4749\n",
      "step: 28, loss_value: 2238.5361, content_loss: 15081.3330, style_loss:   1.4608\n",
      "step: 29, loss_value: 2159.3589, content_loss: 14452.8984, style_loss:   1.4281\n",
      "step: 30, loss_value: 2109.6055, content_loss: 13856.4844, style_loss:   1.4479\n",
      "step: 31, loss_value: 2023.4761, content_loss: 13355.0166, style_loss:   1.3759\n",
      "step: 32, loss_value: 1955.3445, content_loss: 12868.2207, style_loss:   1.3370\n",
      "step: 33, loss_value: 1870.7874, content_loss: 12432.6445, style_loss:   1.2550\n",
      "step: 34, loss_value: 1821.2095, content_loss: 12018.9014, style_loss:   1.2386\n",
      "step: 35, loss_value: 1786.4480, content_loss: 11630.3633, style_loss:   1.2468\n",
      "step: 36, loss_value: 1778.3438, content_loss: 11293.5342, style_loss:   1.2980\n",
      "step: 37, loss_value: 1861.8594, content_loss: 10939.9160, style_loss:   1.5357\n",
      "step: 38, loss_value: 1711.1528, content_loss: 10690.9297, style_loss:   1.2841\n",
      "step: 39, loss_value: 1649.1887, content_loss: 10443.9473, style_loss:   1.2096\n",
      "step: 40, loss_value: 1606.2041, content_loss: 10200.5771, style_loss:   1.1723\n",
      "step: 41, loss_value: 1612.8759, content_loss: 9994.0898, style_loss:   1.2269\n",
      "step: 42, loss_value: 1555.2936, content_loss: 9782.9844, style_loss:   1.1540\n",
      "step: 43, loss_value: 1512.8435, content_loss: 9613.7363, style_loss:   1.1029\n",
      "step: 44, loss_value: 1479.4742, content_loss: 9423.2344, style_loss:   1.0743\n",
      "step: 45, loss_value: 1445.8643, content_loss: 9233.2031, style_loss:   1.0451\n",
      "step: 46, loss_value: 1444.3978, content_loss: 9068.1484, style_loss:   1.0752\n",
      "step: 47, loss_value: 1438.6995, content_loss: 8865.6924, style_loss:   1.1043\n",
      "step: 48, loss_value: 1455.9000, content_loss: 8719.5449, style_loss:   1.1679\n",
      "step: 49, loss_value: 1482.1555, content_loss: 8557.1660, style_loss:   1.2529\n",
      "step: 50, loss_value: 1364.6819, content_loss: 8451.3203, style_loss:   1.0391\n",
      "step: 51, loss_value: 1376.1084, content_loss: 8327.8799, style_loss:   1.0866\n",
      "step: 52, loss_value: 1334.4919, content_loss: 8219.8389, style_loss:   1.0250\n",
      "step: 53, loss_value: 1338.0654, content_loss: 8144.8525, style_loss:   1.0472\n",
      "step: 54, loss_value: 1284.6779, content_loss: 8035.9727, style_loss:   0.9622\n",
      "step: 55, loss_value: 1287.9141, content_loss: 7944.2695, style_loss:   0.9870\n",
      "step: 56, loss_value: 1282.4088, content_loss: 7837.1577, style_loss:   0.9974\n",
      "step: 57, loss_value: 1268.8102, content_loss: 7746.6284, style_loss:   0.9883\n",
      "step: 58, loss_value: 1254.8379, content_loss: 7620.5664, style_loss:   0.9856\n",
      "step: 59, loss_value: 1241.9175, content_loss: 7548.3521, style_loss:   0.9742\n",
      "step: 60, loss_value: 1268.2722, content_loss: 7458.5879, style_loss:   1.0448\n",
      "step: 61, loss_value: 1220.6667, content_loss: 7389.6919, style_loss:   0.9634\n",
      "step: 62, loss_value: 1228.3224, content_loss: 7285.7744, style_loss:   0.9995\n",
      "step: 63, loss_value: 1191.4934, content_loss: 7234.2124, style_loss:   0.9361\n",
      "step: 64, loss_value: 1191.7710, content_loss: 7163.5874, style_loss:   0.9508\n",
      "step: 65, loss_value: 1145.9108, content_loss: 7088.7764, style_loss:   0.8741\n",
      "step: 66, loss_value: 1167.3964, content_loss: 7017.9062, style_loss:   0.9312\n",
      "step: 67, loss_value: 1162.4900, content_loss: 6930.9985, style_loss:   0.9388\n",
      "step: 68, loss_value: 1186.6770, content_loss: 6879.2861, style_loss:   0.9975\n",
      "step: 69, loss_value: 1254.2241, content_loss: 6779.0688, style_loss:   1.1526\n",
      "step: 70, loss_value: 1210.5674, content_loss: 6760.7104, style_loss:   1.0690\n",
      "step: 71, loss_value: 1155.3566, content_loss: 6726.8291, style_loss:   0.9653\n",
      "step: 72, loss_value: 1133.9104, content_loss: 6709.7339, style_loss:   0.9259\n",
      "step: 73, loss_value: 1155.5906, content_loss: 6710.6631, style_loss:   0.9690\n",
      "step: 74, loss_value: 1135.4576, content_loss: 6679.9775, style_loss:   0.9349\n",
      "step: 75, loss_value: 1108.2889, content_loss: 6679.1123, style_loss:   0.8808\n",
      "step: 76, loss_value: 1095.5830, content_loss: 6641.9268, style_loss:   0.8628\n",
      "step: 77, loss_value: 1076.9766, content_loss: 6595.7505, style_loss:   0.8348\n",
      "step: 78, loss_value: 1069.0511, content_loss: 6548.6060, style_loss:   0.8284\n",
      "step: 79, loss_value: 1076.6952, content_loss: 6468.1826, style_loss:   0.8598\n",
      "step: 80, loss_value: 1076.3187, content_loss: 6406.3267, style_loss:   0.8714\n",
      "step: 81, loss_value: 1130.7898, content_loss: 6311.3276, style_loss:   0.9993\n",
      "step: 82, loss_value: 1111.8801, content_loss: 6278.0669, style_loss:   0.9681\n",
      "step: 83, loss_value: 1104.8746, content_loss: 6197.4878, style_loss:   0.9703\n",
      "step: 84, loss_value: 1037.7765, content_loss: 6159.2461, style_loss:   0.8437\n",
      "step: 85, loss_value: 1059.2299, content_loss: 6137.5278, style_loss:   0.8910\n",
      "step: 86, loss_value: 1077.5116, content_loss: 6081.5483, style_loss:   0.9387\n",
      "step: 87, loss_value: 1029.8483, content_loss: 6063.9077, style_loss:   0.8469\n",
      "step: 88, loss_value: 1007.5787, content_loss: 6019.2808, style_loss:   0.8113\n",
      "step: 89, loss_value: 1018.9709, content_loss: 5968.4438, style_loss:   0.8443\n",
      "step: 90, loss_value: 1003.2795, content_loss: 5930.9170, style_loss:   0.8204\n",
      "step: 91, loss_value: 999.7827, content_loss: 5869.0767, style_loss:   0.8258\n",
      "step: 92, loss_value: 987.8544, content_loss: 5830.4243, style_loss:   0.8096\n",
      "step: 93, loss_value: 994.2715, content_loss: 5769.8276, style_loss:   0.8346\n",
      "step: 94, loss_value: 968.8236, content_loss: 5725.9731, style_loss:   0.7925\n",
      "step: 95, loss_value: 952.2184, content_loss: 5668.7793, style_loss:   0.7707\n",
      "step: 96, loss_value: 945.8721, content_loss: 5621.5610, style_loss:   0.7674\n",
      "step: 97, loss_value: 935.4697, content_loss: 5575.4653, style_loss:   0.7558\n",
      "step: 98, loss_value: 934.2801, content_loss: 5522.6973, style_loss:   0.7640\n",
      "step: 99, loss_value: 942.2136, content_loss: 5462.1914, style_loss:   0.7920\n",
      "step: 100, loss_value: 966.5028, content_loss: 5428.1631, style_loss:   0.8474\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        loss_value, content_loss_value, style_loss_value, _ = \\\n",
    "            sess.run([loss, content_loss, style_loss, train_op],\n",
    "                     feed_dict = {\n",
    "                        content: content_value,\n",
    "                         style: style_value\n",
    "                     })\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' \\\n",
    "              % (step + 1,\n",
    "                 loss_value[0],\n",
    "                 content_loss_value[0],\n",
    "                 style_loss_value[0]))\n",
    "        result_img_path = os.path.join(\n",
    "            output_dir, 'result-%05d.jpg' % (step + 1)\n",
    "        )\n",
    "        result_val = result.eval(sess)[0]\n",
    "        result_val = np.clip(result_val, 0, 255)\n",
    "        img_arr = np.asarray(result_val, np.uint8)\n",
    "        img = Image.fromarray(img_arr)\n",
    "        img.save(result_img_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-71e1ac8c",
   "language": "python",
   "display_name": "PyCharm (MachineLearning)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}